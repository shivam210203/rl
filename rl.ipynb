{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8F47a06XvsG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o3vGp6Q1Xxi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o-crqoY9XxbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pRWLGV3_XxYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKhEgLt3XxVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KfC0Pn9JXxSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nWkHN3arXxO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_cdnMaOXxL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I999aXFUXxIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "69zE42n0XxGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QiVMel5MXxDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "408KKX3YXxAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k1M4Yo7DXw9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yw_QqUJIXw6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aj0baL6ZXw3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kcJl3M4EXyr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Markov Decision Process (MDP) is a fully observable, probabilistic state model. The most common formulation of MDPs is a Discounted-Reward Markov Decision Process. A discount-reward MDP is a tuple\n",
        " containing:\n",
        "\n",
        "a state space\n",
        ";\n",
        "\n",
        "initial state\n",
        ";\n",
        "\n",
        "actions\n",
        " applicable in each state\n",
        " that our agent can execute;\n",
        "\n",
        "transition probabilities\n",
        " for\n",
        " and\n",
        ";\n",
        "\n",
        "rewards\n",
        " positive or negative of transitioning from state\n",
        " to state\n",
        " using action\n",
        "; and\n",
        "\n",
        "a discount factor\n",
        ".\n",
        "A multi-armed bandit (also known as an\n",
        "-armed bandit) is defined by a set of random variables\n",
        " where:\n",
        "\n",
        ", such that\n",
        " is the arm of the bandit; and\n",
        "\n",
        " the index of the play of arm\n",
        ";\n",
        "\n",
        "Successive plays\n",
        " are assumed to be independently distributed, but we do not know the probability distributions of the random variables.\n",
        "\n",
        "The idea is that a gambler iteratively plays rounds, observing the reward from the arm after each round, and can adjust their strategy each time. The aim is to maximise the sum of the rewards collected over all rounds.\n",
        "\n",
        "Multi-arm bandit strategies aim to learn a policy\n",
        ", where\n",
        " is the play.\n",
        "\n",
        " Given that we do not know the probability distributions, a simple strategy is simply to select the arm given a uniform distribution; that is, select each arm with the same probability. This is just uniform sampling.\n",
        "\n",
        "Then, the Q-value for an action\n",
        " can be estimated using the following formula:\n",
        "\n",
        "\n",
        "\n",
        "where\n",
        " is the number of rounds so far,\n",
        " is the number of times\n",
        " selected in previous rounds, and\n",
        " is the reward obtained in the\n",
        "-th round for playing arm\n",
        "..\n",
        "\n",
        "The idea here is that for a multi-armed bandit problem, we explore the options uniformly for some time, and then once we are confident we have enough samples (when the changes to the values of\n",
        " start to stabilise), we start selecting\n",
        ". This is known as the\n",
        "-first strategy, where the parameter\n",
        " (epsilon), determines how many rounds to select random actions before moving to the greedy action.\n",
        "\n",
        "But what is the issue? Time is wasted equally in all actions using the uniform distribution. Instead, we can focus on the most promising actions given the rewards we have received so far."
      ],
      "metadata": {
        "id": "lvCUmzV-hODD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "from qtable import QTable\n",
        "\n",
        "\n",
        "\"\"\" Run a bandit algorithm for a number of episodes, with each episode\n",
        "being a set length.\n",
        "\"\"\"\n",
        "\n",
        "def run_bandit(bandit, episodes=200, episode_length=500, drift=True):\n",
        "\n",
        "    # The actions available\n",
        "    actions = [0, 1, 2, 3, 4]\n",
        "\n",
        "    # A dummy state\n",
        "    state = 1\n",
        "\n",
        "    rewards = []\n",
        "    for _ in range(0, episodes):\n",
        "        bandit.reset()\n",
        "\n",
        "        # The probability of receiving a payoff of 1 for each action\n",
        "        probabilities = [0.1, 0.3, 0.7, 0.2, 0.1]\n",
        "\n",
        "        # The number of times each arm has been selected\n",
        "        times_selected = defaultdict(lambda: 0)\n",
        "        qtable = QTable()\n",
        "\n",
        "        episode_rewards = []\n",
        "        for step in range(0, episode_length):\n",
        "\n",
        "            # Halfway through the episode, change the probabilities\n",
        "            if drift and step == episode_length / 2:\n",
        "                probabilities = [0.5, 0.2, 0.0, 0.3, 0.3]\n",
        "\n",
        "            # Select an action using the bandit\n",
        "            action = bandit.select(state, actions, qtable)\n",
        "\n",
        "            # Get the reward for that action\n",
        "            reward = 0\n",
        "            if random.random() < probabilities[action]:\n",
        "                reward = 5\n",
        "\n",
        "            episode_rewards += [reward]\n",
        "\n",
        "            times_selected[action] = times_selected[action] + 1\n",
        "            qtable.update(\n",
        "                state,\n",
        "                action,\n",
        "                (reward / times_selected[action])\n",
        "                - (qtable.get_q_value(state, action) / times_selected[action]),\n",
        "            )\n",
        "\n",
        "        rewards += [episode_rewards]\n",
        "\n",
        "    return rewards"
      ],
      "metadata": {
        "id": "NbjWHYpmi2k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "monte carlo"
      ],
      "metadata": {
        "id": "CpLO6NjVjTB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GWLJFB-Pl5f3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qtOFPhTynSgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "from plot_utils import plot_blackjack_values, plot_policy\n",
        "env = gym.make('Blackjack-v0')\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "\n",
        "for i_episode in range(3):\n",
        "    state = env.reset()\n",
        "    while True:\n",
        "        print(state)\n",
        "        action = env.action_space.sample()\n",
        "        state, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            print('End game! Reward: ', reward)\n",
        "            print('You won :)\\n') if reward > 0 else print('You lost :(\\n')\n",
        "            break\n",
        "\n",
        "def generate_episode_from_limit_stochastic(bj_env):\n",
        "    episode = []\n",
        "    state = bj_env.reset()\n",
        "    while True:\n",
        "        probs = [0.8, 0.2] if state[0] > 18 else [0.2, 0.8]\n",
        "        action = np.random.choice(np.arange(2), p=probs)\n",
        "        next_state, reward, done, info = bj_env.step(action)\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    return episode\n",
        "\n",
        "for i in range(3):\n",
        "    print(generate_episode_from_limit_stochastic(env))\n",
        "\n",
        "def mc_prediction_q(env, num_episodes, generate_episode, gamma=1.0):\n",
        "    # initialize empty dictionaries of arrays\n",
        "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    # loop over episodes\n",
        "    for i_episode in range(1, num_episodes+1):\n",
        "        # monitor progress\n",
        "        if i_episode % 1000 == 0:\n",
        "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
        "            sys.stdout.flush()\n",
        "        # generate an episode\n",
        "        episode = generate_episode(env)\n",
        "        # obtain the states, actions, and rewards\n",
        "        states, actions, rewards = zip(*episode)\n",
        "        # prepare for discounting\n",
        "        discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
        "        # update the sum of the returns, number of visits, and action-value\n",
        "        # function estimates for each state-action pair in the episode\n",
        "        for i, state in enumerate(states):\n",
        "            returns_sum[state][actions[i]] += sum(rewards[i:]*discounts[:-(1+i)])\n",
        "            N[state][actions[i]] += 1.0\n",
        "            Q[state][actions[i]] = returns_sum[state][actions[i]] / N[state][actions[i]]\n",
        "    return Q\n",
        "\n",
        "\n",
        "# obtain the action-value function\n",
        "Q = mc_prediction_q(env, 500000, generate_episode_from_limit_stochastic)\n",
        "\n",
        "# obtain the corresponding state-value function\n",
        "V_to_plot = dict((k,(k[0]>18)*(np.dot([0.8, 0.2],v)) + (k[0]<=18)*(np.dot([0.2, 0.8],v))) \\\n",
        "         for k, v in Q.items())\n",
        "\n",
        "# plot the state-value function\n",
        "plot_blackjack_values(V_to_plot)\n",
        "\n"
      ],
      "metadata": {
        "id": "BnodvI8Fl5sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic programming"
      ],
      "metadata": {
        "id": "FHuL5sqgnTjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZEQcUsy_nTr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "import check_test\n",
        "from frozenlake import FrozenLakeEnv\n",
        "from plot_utils import plot_values\n",
        "\n",
        "env = FrozenLakeEnv()\n",
        "\n",
        "# print the state space and action space\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "\n",
        "# print the total number of states and actions\n",
        "print(env.nS)\n",
        "print(env.nA)\n",
        "\n",
        "env.P[1][0]\n",
        "def policy_evaluation(env, policy, gamma=1, theta=1e-8):\n",
        "    V = np.zeros(env.nS)\n",
        "\n",
        "    ## TODO: complete the function\n",
        "\n",
        "    return V\n",
        "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
        "\n",
        "# evaluate the policy\n",
        "V = policy_evaluation(env, random_policy)\n",
        "\n",
        "plot_values(V)\n",
        "\n",
        "\n",
        "check_test.run_check('policy_evaluation_check', policy_evaluation)\n",
        "\n"
      ],
      "metadata": {
        "id": "fxnSLTTZn4G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R42O6v3Ro_VA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import inspect\n",
        "%matplotlib inline\n",
        "import math\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "%matplotlib inline\n",
        "\n",
        "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
        "parentdir = os.path.dirname(currentdir)\n",
        "os.sys.path.insert(0,parentdir)\n",
        "\n",
        "from utils import plotting\n",
        "class gridworld:\n",
        "    def __init__(self,dims,movementReward,terminalStates,gamma=1):\n",
        "        self.dims = dims\n",
        "        self.movementReward = movementReward\n",
        "        self.terminalStates = terminalStates\n",
        "        self.stateValues = np.zeros([dims,dims])\n",
        "        self.gamma = gamma\n",
        "\n",
        "class policyEval(gridworld):\n",
        "    def __init__(self,prob=[.25,.25,.25,.25],dims=4,movementReward=-1,terminalStates = [[0,0],[3,3]]):\n",
        "        assert not any(n < 0 for n in prob), 'probabilities are always non-negative dammit!'\n",
        "        assert np.sum(prob)==1,'sum is not 1'\n",
        "\n",
        "        super(policyEval,self).__init__(dims,movementReward,terminalStates)\n",
        "        #self.currentValues = np.zeros([dims,dims])\n",
        "        self.action_prob = {'UP':prob[0],'DOWN':prob[1],'LEFT':prob[2],'RIGHT':prob[3]}\n",
        "\n",
        "\n",
        "\n",
        "    def getNextState(self,currentState,action,dims):\n",
        "        if action == 'UP':\n",
        "            nextState = (currentState[0]-1,currentState[1])\n",
        "        if action == 'DOWN':\n",
        "            nextState = (currentState[0]+1,currentState[1])\n",
        "        if action == 'LEFT':\n",
        "            nextState = (currentState[0],currentState[1]-1)\n",
        "        if action == 'RIGHT':\n",
        "            nextState = (currentState[0],currentState[1]+1)\n",
        "        nextState = (max(nextState[0],0),max(nextState[1],0))\n",
        "        nextState = (min(nextState[0],dims-1),min(nextState[1],dims-1))\n",
        "        return nextState\n",
        "\n",
        "\n",
        "    def evaluate(self):\n",
        "        theta = 1e-4\n",
        "\n",
        "        while(1):\n",
        "            for i in range(self.dims):\n",
        "                for j in range(self.dims):\n",
        "                    if [i,j] not in self.terminalStates:\n",
        "                        newValue = 0\n",
        "                        for action in self.action_prob.keys():\n",
        "                            nextState = self.getNextState((i,j),action,self.dims)\n",
        "                            newValue += self.action_prob[action]*(self.movementReward + self.gamma*self.stateValues[nextState[0]][nextState[1]])\n",
        "\n",
        "                        if abs(self.stateValues[i][j] - newValue) < theta:\n",
        "                            self.stateValues[i][j] = newValue\n",
        "                            return self.stateValues\n",
        "                        self.stateValues[i][j] = newValue\n",
        "\n",
        "\n",
        "\n",
        "uniformDistributionPolicy = policyEval()\n",
        "plotting.plot_gridworld_values(uniformDistributionPolicy.evaluate())\n",
        "\n"
      ],
      "metadata": {
        "id": "uzL8GUr4o_p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q learning"
      ],
      "metadata": {
        "id": "SUG75qTVqqVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapted from: https://github.com/lazyprogrammer/machine_learning_examples/tree/master/rl\n",
        "# the Q-Learning method to find the optimal policy and value function\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from gridWorldGame import standard_grid, negative_grid,print_values, print_policy\n",
        "\n",
        "SMALL_ENOUGH = 1e-3\n",
        "GAMMA = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "ALPHA = 0.1\n",
        "\n",
        "def max_dict(d):\n",
        "  # returns the argmax (key) and max (value) from a dictionary\n",
        "  max_key = None\n",
        "  max_val = float('-inf')\n",
        "  for k, v in d.items():\n",
        "    if v > max_val:\n",
        "      max_val = v\n",
        "      max_key = k\n",
        "  return max_key, max_val\n",
        "\n",
        "def random_action(a, eps=0.1):\n",
        "  # epsilon-soft to ensure all states are visited\n",
        "  p = np.random.random()\n",
        "  if p < (1 - eps):\n",
        "    return a\n",
        "  else:\n",
        "    return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "grid = negative_grid(step_cost=-0.1)\n",
        "\n",
        "# print rewards\n",
        "print(\"rewards:\")\n",
        "print_values(grid.rewards, grid)\n",
        "\n",
        "# no policy initialization, policy is derived from most recent Q like SARSA\n",
        "\n",
        "# initialize Q(s,a)\n",
        "Q = {}\n",
        "states = grid.all_states()\n",
        "for s in states:\n",
        "  Q[s] = {}\n",
        "  for a in ALL_POSSIBLE_ACTIONS:\n",
        "    Q[s][a] = 0\n",
        "\n",
        "# initial Q values for all states in grid\n",
        "print(Q)\n",
        "\n",
        "update_counts = {}\n",
        "update_counts_sa = {}\n",
        "for s in states:\n",
        "  update_counts_sa[s] = {}\n",
        "  for a in ALL_POSSIBLE_ACTIONS:\n",
        "    update_counts_sa[s][a] = 1.0\n",
        "\n",
        "# repeat until convergence\n",
        "t = 1.0\n",
        "deltas = []\n",
        "for it in range(10000):\n",
        "  if it % 100 == 0:\n",
        "    t += 1e-2\n",
        "  if it % 2000 == 0:\n",
        "    print(\"iteration:\", it)\n",
        "\n",
        "  # instead of 'generating' an epsiode, we will PLAY\n",
        "  # an episode within this loop\n",
        "  s = (2, 0) # start state\n",
        "  grid.set_state(s)\n",
        "\n",
        "  # the first (s, r) tuple is the state we start in and 0\n",
        "  # (since we don't get a reward) for simply starting the game\n",
        "  # the last (s, r) tuple is the terminal state and the final reward\n",
        "  # the value for the terminal state is by definition 0, so we don't\n",
        "  # care about updating it.\n",
        "  a, _ = max_dict(Q[s])\n",
        "  biggest_change = 0\n",
        "  while not grid.game_over():\n",
        "    a = random_action(a, eps=0.5/t) # epsilon-greedy\n",
        "    # random action also works, but slower since you can bump into walls\n",
        "    # a = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "    r = grid.move(a)\n",
        "    s2 = grid.current_state()\n",
        "\n",
        "    # adaptive learning rate\n",
        "    alpha = ALPHA / update_counts_sa[s][a]\n",
        "    update_counts_sa[s][a] += 0.005\n",
        "\n",
        "    # we will update Q(s,a) AS we experience the episode\n",
        "    old_qsa = Q[s][a]\n",
        "    # the difference between SARSA and Q-Learning is with Q-Learning\n",
        "    # we will use this max[a']{ Q(s',a')} in our update\n",
        "    # even if we do not end up taking this action in the next step\n",
        "    a2, max_q_s2a2 = max_dict(Q[s2])\n",
        "    Q[s][a] = Q[s][a] + alpha*(r + GAMMA*max_q_s2a2 - Q[s][a])\n",
        "    biggest_change = max(biggest_change, np.abs(old_qsa - Q[s][a]))\n",
        "\n",
        "    # we would like to know how often Q(s) has been updated too\n",
        "    update_counts[s] = update_counts.get(s,0) + 1\n",
        "\n",
        "    # next state becomes current state\n",
        "    s = s2\n",
        "    a = a2\n",
        "\n",
        "  deltas.append(biggest_change)\n",
        "\n"
      ],
      "metadata": {
        "id": "V5BQ02uhqqwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aHNKRayPrHUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate theory on q learning\n",
        "\n",
        "# Theory on Q-Learning\n",
        "\n",
        "# Q-learning is a model-free reinforcement learning algorithm that learns an optimal policy for an agent interacting with an environment.  It does so by estimating the optimal action-value function, Q*(s, a), which represents the expected cumulative discounted reward for taking action 'a' in state 's' and following the optimal policy thereafter.\n",
        "\n",
        "# Key Concepts:\n",
        "\n",
        "# 1. State-Action Value Function (Q-value):  Q(s, a) estimates the long-term value of taking action 'a' in state 's'.  The goal of Q-learning is to approximate Q*(s, a).\n",
        "\n",
        "# 2. Exploration vs. Exploitation: Q-learning must balance exploration (trying out new actions) and exploitation (choosing the action with the highest estimated Q-value).  Common methods for balancing these include epsilon-greedy strategies.\n",
        "\n",
        "# 3. Temporal Difference (TD) Learning: Q-learning updates Q-values based on observed transitions and rewards.  It uses a TD target, which is an estimate of the optimal Q-value based on the current Q-value and the reward received for taking an action. The difference between the current Q-value and the TD target is the TD error.\n",
        "\n",
        "# 4. Update Rule: The core of Q-learning is the update rule, which iteratively refines the Q-value estimates. The update rule typically follows this form:\n",
        "\n",
        "#    Q(s, a) = Q(s, a) + alpha * [r + gamma * max_a' Q(s', a') - Q(s, a)]\n",
        "\n",
        "#    where:\n",
        "#       - alpha is the learning rate, controlling the step size of the update.\n",
        "#       - r is the immediate reward received for taking action 'a' in state 's'.\n",
        "#       - gamma is the discount factor, determining the importance of future rewards.\n",
        "#       - s' is the next state after taking action 'a' in state 's'.\n",
        "#       - max_a' Q(s', a') is the maximum estimated Q-value for the next state s', across all possible actions a'.  This is the crucial difference between SARSA and Q-Learning.\n",
        "\n",
        "# 5. Off-policy Learning: Q-learning is an off-policy algorithm, meaning that it learns about the optimal policy while following a different (e.g., epsilon-greedy) behavior policy. This allows it to explore more effectively.  SARSA is on-policy.\n",
        "\n",
        "# 6. Convergence: Under certain conditions (e.g., appropriate learning rate, sufficient exploration), Q-learning converges to the optimal Q-value function Q*(s, a).\n",
        "\n",
        "\n",
        "# Differences between Q-learning and SARSA:\n",
        "\n",
        "#  - Q-learning uses the maximum estimated Q-value of the *next* state (max_a' Q(s', a')) in its update rule.\n",
        "#  - SARSA uses the Q-value of the *actual* action taken in the *next* state.  SARSA is on-policy.\n",
        "#  - As a result, Q-learning is more optimistic and tends to explore more.\n",
        "\n",
        "\n",
        "# Example Implementation Notes from the Provided Code:\n",
        "\n",
        "# - The code demonstrates Q-learning applied to a grid world environment and a Blackjack environment.\n",
        "# - It uses an epsilon-greedy policy for action selection.\n",
        "# - The learning rate is adapted based on the number of visits to each state-action pair.\n",
        "# - The code visualizes the learned value function and/or policy.\n",
        "\n",
        "# Further improvements:\n",
        "\n",
        "# - Experience Replay\n",
        "# - Prioritized Experience Replay\n",
        "# - Double Q-Learning\n",
        "# - Dueling Deep Q Networks\n"
      ],
      "metadata": {
        "id": "wuPF29ilrH1s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}